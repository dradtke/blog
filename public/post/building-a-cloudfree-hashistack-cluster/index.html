<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.115.1">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Building a Cloud™-Free Hashistack Cluster 🌥 &middot; Version 7.0</title>

  
  <link type="text/css" rel="stylesheet" href="https://damienradtke.com/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://damienradtke.com/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://damienradtke.com/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://damienradtke.com/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://damienradtke.com/"><h1>Version 7.0</h1></a>
      <p class="lead">
       Fixes many known issues with version 6.0 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://damienradtke.com/">Home</a> </li>
        <li><a href="/pages/about"> About </a></li><li><a href="https://github.com/dradtke/"> Projects — Github </a></li><li><a href="https://git.sr.ht/~damien"> Projects — Sourcehut </a></li>
      </ul>
    </nav>

    <p>&copy; 2023. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
    <h1>Building a Cloud™-Free Hashistack Cluster 🌥</h1>
    
      <time datetime=2020-09-12T00:00:00Z class="post-date">Sat, Sep 12, 2020</time>
    

    <h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#preface">Preface</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#safety-first-tls">Safety First: TLS</a></li>
<li><a href="#behind-the-firewall">Behind the Firewall</a></li>
<li><a href="#provisioning-with-terraform">Provisioning With Terraform</a></li>
<li><a href="#running-a-website">Running a Website</a></li>
<li><a href="#final-thoughts">Final Thoughts</a></li>
</ol>
<h2 id="preface">Preface</h2>
<p>&ldquo;Hashistack&rdquo; refers to a network cluster based on <a href="https://www.hashicorp.com/">HashiCorp</a> tools, and
after off-and-on spending a considerable amount of time on it, the architecture of my own cluster
(on which this blog is running, among other personal projects) has finally (mostly) stabilized. In
this post I will walk you through its high-level structure, some of the benefits provided, and
hopefully show you how you can build a similar cluster for your personal or professional projects.</p>
<p>Everything I discuss here is available publicly in my <a href="https://git.sr.ht/~damien/infrastructure/">infrastructure
repo</a>. I reference it frequently, and some may find it
helpful simply to browse through that code.</p>
<h2 id="the-cloud">The Cloud™</h2>
<p>The term &ldquo;cloud&rdquo; is not very well-defined, but usually refers to those platforms that provide
managed services beyond Virtual Private Server (VPS) provisioning. In this post, I want to draw a
distinction between VPS providers and Cloud™ providers. While there are many different VPS
providers, here I use the term Cloud™ to refer to the big 3: Amazon Web Services, Microsoft Azure,
and Google Cloud Platform.</p>
<p>There is nothing inherently wrong with building applications on the Cloud™, but it&rsquo;s an area that is
already heavily discussed and supported, especially with the recent launch of <a href="https://www.hashicorp.com/cloud-platform/">HashiCorp Cloud
Platform</a>. The benefit of these services is that they let
you get up and running quickly, but they also often come with a hefty price tag, and can make it
harder to switch providers in the future for cost or other reasons. Using VPS providers can save you
money, makes it easier to switch and, in my opinion, is more fun.</p>
<p>My cluster is built on <a href="https://www.linode.com/">Linode</a> because they offer openSUSE VPS images and
DNS management. However, this guide should still be relevant no matter what distribution you&rsquo;re
using, though with some extra steps if you do not have
<a href="https://www.freedesktop.org/wiki/Software/systemd/"><code>systemd</code></a> or
<a href="https://firewalld.org/"><code>firewalld</code></a> available.</p>
<h2 id="getting-started">Getting Started</h2>
<h3 id="create-a-support-server">Create a Support Server</h3>
<p>A primary fixture in my setup is the use of a &ldquo;support&rdquo; server, which is a single VPS instance that
acts as the entrypoint for the rest of the cluster. Most of the infrastructure is provisioned with
Terraform and is designed to be easily replaceable; the support server is the lone instance which is
cared for as a <a href="http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/">pet</a>
rather than cattle. This is very similar in concept to a bastion server, but with less of a focus on
security, and more on cost savings and functionality.</p>
<p>The support server&rsquo;s functions include:</p>
<ol>
<li>Cluster members are provisioned with a random root password which is never exposed; <strong>access is
only granted via SSH public keys</strong>, and never to <code>root</code> (after provisioning has finished).
Restricting authorized keys to only what is available on the support server is an easy way to
tighten your security. (My setup is actually slightly different in that servers only allow access
with the public keys defined in Linode and I always forward my SSH agent to the support server,
but I still do all cluster operations on the support server.)</li>
<li>The support server acts as the <strong>guardian of the Certificate Authorities</strong>, and new certificates
are only issued by making a request to the support server.</li>
<li>The support server <strong>maintains Terraform state</strong>. Setting up a backend is an option here as well,
but for relatively simple uses like mine, it&rsquo;s easier to stick with the &ldquo;local&rdquo; backend on the
support server.</li>
<li><strong>Cheap artifact hosting</strong>. As long as you have a server running with a known address, you can have
your support server host all your artifacts and serve them with <a href="https://min.io/">minio</a> or even
a plain HTTP server.</li>
</ol>
<h3 id="a-note-on-ipv6">A Note on IPv6</h3>
<p>Where possible, everything is configured to communicate over IPv6. Despite its slow adoption, IPv6
is a good choice here because it is more efficient, opens up another possible route for cost savings
due to the scarcity of IPv4 addresses, and VPS providers are more likely to support it than Internet
Service Providers anyway.</p>
<h2 id="safety-first-tls">Safety First: TLS</h2>
<p>In order to safely restrict access to cluster resources, the first step you&rsquo;ll want to take with
your support server is to generate Certificate Authorities that can be used to configure TLS for
each of the services. My setup largely follows the approach outlined in HashiCorp&rsquo;s guide to
<a href="https://learn.hashicorp.com/nomad/transport-security/enable-tls">enabling TLS for Nomad</a>, which
will go more in-depth in how to use <code>cfssl</code> to get set up.</p>
<p>It might be overkill, but I use a different CA for each service, and they are stored on the support
server under <code>/etc/ssl</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>/etc/ssl
</span></span><span style="display:flex;"><span>├── consul
</span></span><span style="display:flex;"><span>│   ├── ca-key.pem
</span></span><span style="display:flex;"><span>│   └── ca.pem
</span></span><span style="display:flex;"><span>├── nomad
</span></span><span style="display:flex;"><span>│   ├── ca-key.pem
</span></span><span style="display:flex;"><span>│   └── ca.pem
</span></span><span style="display:flex;"><span>└── vault
</span></span><span style="display:flex;"><span>    ├── ca-key.pem
</span></span><span style="display:flex;"><span>    └── ca.pem</span></span></code></pre></div>
<p>Another important security note is that the key permissions should be as restrictive as possible:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>-r-------- 1 root root  227 Jul 23  2019 /etc/ssl/consul/ca-key.pem
</span></span><span style="display:flex;"><span>-r--r--r-- 1 root root 1249 Jul 23  2019 /etc/ssl/consul/ca.pem</span></span></code></pre></div>
<h3 id="cfssl-configuration">CFSSL Configuration</h3>
<p>CFSSL is a general-purpose CLI tool for managing TLS files, but it also has the
ability to run a server process for handling new certificate requests. That
requires defining a configuration file at <code>/etc/ssl/cfssl.json</code> on the support server:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;signing&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;default&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;expiry&#34;</span>: <span style="color:#e6db74">&#34;87600h&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;usages&#34;</span>: [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;signing&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;key encipherment&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;server auth&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;client auth&#34;</span>
</span></span><span style="display:flex;"><span>      ],
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;auth_key&#34;</span>: <span style="color:#e6db74">&#34;primary&#34;</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  },
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;auth_keys&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;primary&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;standard&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;key&#34;</span>: <span style="color:#e6db74">&#34;...&#34;</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>The <code>primary</code> auth key here must be a 16-bit hex value, and is used to prevent unauthorized parties
from requesting certificates. All new certificate requests effectively use that key as a password,
so treat it just like you would treat your private keys by <em>never</em> checking it into source control.
For more details on CFSSL configuration, see CloudFlare&rsquo;s post on <a href="https://blog.cloudflare.com/how-to-build-your-own-public-key-infrastructure/">building your own public key
infrastructure</a>.</p>
<p>There is one other tool that CFSSL provides but isn&rsquo;t mentioned in the article called <code>multirootca</code>,
which is effectively just a multiplexer for CFSSL. By default, the CFSSL server will only issue
certificates for a single Certificate Authority; <code>multirootca</code> lets you run the server in a way that
supports multiple authorities. It requires its own configuration file, but a very simple one:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-conf" data-lang="conf"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">[</span> consul <span style="color:#960050;background-color:#1e0010">]</span>
</span></span><span style="display:flex;"><span>private <span style="color:#f92672">=</span> file<span style="color:#960050;background-color:#1e0010">:///</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>consul<span style="color:#960050;background-color:#1e0010">/</span>ca-key.pem
</span></span><span style="display:flex;"><span>certificate <span style="color:#f92672">=</span> <span style="color:#960050;background-color:#1e0010">/</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>consul<span style="color:#960050;background-color:#1e0010">/</span>ca.pem
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> <span style="color:#960050;background-color:#1e0010">/</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>cfssl.json
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">[</span> vault <span style="color:#960050;background-color:#1e0010">]</span>
</span></span><span style="display:flex;"><span>private <span style="color:#f92672">=</span> file<span style="color:#960050;background-color:#1e0010">:///</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>vault<span style="color:#960050;background-color:#1e0010">/</span>ca-key.pem
</span></span><span style="display:flex;"><span>certificate <span style="color:#f92672">=</span> <span style="color:#960050;background-color:#1e0010">/</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>vault<span style="color:#960050;background-color:#1e0010">/</span>ca.pem
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> <span style="color:#960050;background-color:#1e0010">/</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>cfssl.json
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">[</span> nomad <span style="color:#960050;background-color:#1e0010">]</span>
</span></span><span style="display:flex;"><span>private <span style="color:#f92672">=</span> file<span style="color:#960050;background-color:#1e0010">:///</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>nomad<span style="color:#960050;background-color:#1e0010">/</span>ca-key.pem
</span></span><span style="display:flex;"><span>certificate <span style="color:#f92672">=</span> <span style="color:#960050;background-color:#1e0010">/</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>nomad<span style="color:#960050;background-color:#1e0010">/</span>ca.pem
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> <span style="color:#960050;background-color:#1e0010">/</span>etc<span style="color:#960050;background-color:#1e0010">/</span>ssl<span style="color:#960050;background-color:#1e0010">/</span>cfssl.json</span></span></code></pre></div>
<p>The <code>multirootca</code>
<a href="https://git.sr.ht/~damien/infrastructure/tree/master/services/support/multirootca.service">service</a>
is then run under systemd so that it can keep running in the background, serving incoming
certificate requests.</p>
<p>Issuing new certificates is done from every cluster member via <a href="https://git.sr.ht/~damien/infrastructure/tree/master/scripts/issue-cert.sh">this
script</a>, which uses the
CFSSL CLI to make a <code>gencert</code> request to the running <code>multirootca</code> service on the support server.
Like the support server, certs and keys on cluster members all live under <code>/etc/ssl</code>, grouped by
application name, including the public key for the certificate authority.</p>
<p>One thing to note is how Consul, Nomad, and Vault interact with each other, since that affects which
certificates you need to issue. Vault depends on Consul, and Nomad depends on both Consul and Vault,
so an instance running a Nomad agent will have a lot of certificates in <code>/etc/ssl/nomad</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>-rw-r--r-- 1 nomad nomad 692 Jun 14 21:59 ca.pem
</span></span><span style="display:flex;"><span>-r--r----- 1 nomad nomad 228 Jun 14 22:00 cli-key.pem
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 nomad nomad 714 Jun 14 22:00 cli.pem
</span></span><span style="display:flex;"><span>-r-------- 1 nomad nomad 228 Jun 14 22:00 consul-key.pem
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 nomad nomad 970 Jun 14 22:00 consul.pem
</span></span><span style="display:flex;"><span>-r-------- 1 nomad nomad 228 Jun 15 19:07 nomad-key.pem
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 nomad nomad 803 Jun 15 19:07 nomad.pem
</span></span><span style="display:flex;"><span>-r-------- 1 nomad nomad 228 Jun 14 22:00 vault-key.pem
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 nomad nomad 714 Jun 14 22:00 vault.pem</span></span></code></pre></div>
<h3 id="a-note-on-hostnames">A Note on Hostnames</h3>
<p>While working on this project, the most common TLS-related errors I encountered were &ldquo;unknown
certificate authority&rdquo; and &ldquo;bad hostname.&rdquo; The former is usually pretty easy to fix; just ensure
<code>ca.pem</code> is available on every node and that it&rsquo;s being used as the relevant CA in the configs; but
the latter requires a little more thought.</p>
<p>Every node needs to consider how it is going to be queried. By default, <code>issue-cert.sh</code> considers
only <code>localhost</code> to be a valid hostname, which means that only API requests to <code>localhost</code> will be
accepted, which in turn means that all requests from another location (like the support server) will
be rejected. If you want to query your node using another name, it needs to be included as a valid
hostname when the certificate is issued.</p>
<p>For all nodes, the public IP address is a common alternative hostname to specify. This will let you
query the node from anywhere as long as your CLI is configured with its own valid certificate (a
separate <a href="https://git.sr.ht/~damien/infrastructure/tree/master/tools/issue-cert">script</a> makes this
pretty easy; it&rsquo;s very similar to the one used during node provisioning, but it operates directly on
the CA private key instead of using the remote).</p>
<p>In addition, there are a couple special cases to consider:</p>
<ol>
<li>Consul services should add <code>&lt;name&gt;.service.consul</code> as a valid hostname. Both Nomad and Vault
servers register their own services, so they should add <code>nomad.service.consul</code> and
<code>vault.service.consul</code> respectively.</li>
<li>All Nomad agents, both servers and clients, should add their <a href="https://learn.hashicorp.com/nomad/transport-security/enable-tls#node-certificates">special
hostname</a>,
which is constructed from the agent&rsquo;s role and region. All Nomad agents in my cluster stick with
the default region <code>global</code>, so Nomad servers use <code>server.global.nomad</code> and clients use
<code>client.global.nomad</code>.</li>
</ol>
<h2 id="behind-the-firewall">Behind the Firewall</h2>
<p>With any cluster, a properly-configured firewall is a <em>must</em>. I use
<a href="https://firewalld.org/"><code>firewalld</code></a>, which is the new default for openSUSE, and it&rsquo;s not too
difficult to configure.</p>
<p><code>firewalld</code> defines two important concepts for classifying incoming connections: <strong>services</strong> and
<strong>zones</strong>. Services simply define a list of protocol/port pairs that are identified by a name; for
example, the <code>ssh</code> service would be defined as <code>tcp/22</code>, because it requires TCP connections on port
22. Zones, roughly speaking, are used to classify where a connection is coming from, and what should
be done with it, such as &ldquo;for any connection to one of these services, from one of these IP
addresses, accept it.&rdquo; Connections that aren&rsquo;t explicitly given access will be dropped by default.</p>
<p>The full list of features <code>firewalld</code> provides for zones is outside the scope of this post, and if
you plan to use <code>firewalld</code>, it&rsquo;s probably good to <a href="https://www.linuxjournal.com/content/understanding-firewalld-multi-zone-configurations">read
more</a>.
However, it is still useful even with a very simple configuration.</p>
<p>One benefit of having TLS configured for Consul, Nomad, and Vault is that it is perfectly safe to
open their ports to any incoming connection regardless of source IP, since connections will be
rejected if they do not have a valid client certificate anyway.  There is a lot of room for
flexibility here though, and further restrictions may be wanted if you expect <a href="https://www.youtube.com/watch?v=xpfCr4By71U">sensitive
information</a> to go through your cluster.</p>
<h3 id="creating-a-cluster-only-zone">Creating a Cluster-Only Zone</h3>
<p>The natural fit for a more secure zone is one that only processes requests coming from other nodes
inside your cluster. While my setup leaves many ports open to the world, there is one exception:
Nomad client dynamic ports. While connections to Nomad directly require a client certificate, I
wanted my applications running on Nomad to be able to communicate with each other (more on that
below), and that requires opening up the dynamic port range to the other Nomad clients.</p>
<p>To do this, I created a new service called
<a href="https://git.sr.ht/~damien/infrastructure/tree/master/firewall/services/nomad-dynamic-ports.xml"><code>nomad-dynamic-ports</code></a>
that grants access to the <a href="https://www.nomadproject.io/docs/job-specification/network#dynamic-ports">port
range</a> used by Nomad. All
applications running on Nomad that request a port will be assigned a random one from this range, so
we want to open up the whole range, but <em>only to other Nomad clients</em>.</p>
<p>Each Nomad client is provisioned with a zone called <code>nomad-clients</code>, which allows access to the
<code>nomad-dynamic-ports</code> service, but with no other information, so by default no connections will land
in this zone. In order for it to work, we need to add the IP address of every other Nomad client as
a source to this zone, and to do this for all the clients.</p>
<p>To do this, I wrote a
<a href="https://git.sr.ht/~damien/infrastructure/tree/master/tools/update-nomad-client-firewall">script</a>
that uses Terraform output to get a list of all the Nomad client IP addresses, then SSH on to each
one and make the necessary updates. This script can be run automatically by Terraform with a
<a href="https://registry.terraform.io/providers/hashicorp/null/latest/docs/resources/resource"><code>null_resource</code></a>,
which will help keep things in sync.</p>
<h2 id="provisioning-with-terraform">Provisioning With Terraform</h2>
<p>Terraform was not actually my go-to solution for provisioning. Initially my plan was to stay simple
and stick with scripts like <code>deploy-new-server.sh</code> using Linode&rsquo;s API, but I ended up moving over to
Terraform for one big reason: state management. Terraform&rsquo;s big win is keeping track of what you&rsquo;ve
already deployed, which makes cluster management much easier. In particular, you can provision your
client nodes with <a href="https://git.sr.ht/~damien/infrastructure/tree/c97e0e2b/terraform/nomad-client/main.tf#L70-74">existing
knowledge</a>
of your Consul servers, and write
<a href="https://git.sr.ht/~damien/infrastructure/tree/master/tools/update-nomad-client-firewall">scripts</a>
that can use that knowledge after-the-fact to make additional changes. All of these operations are
much easier with a state management tool than they would be if you had to query your VPS&rsquo; API every
time you wanted to know a node&rsquo;s IP address.</p>
<h3 id="overall-structure">Overall Structure</h3>
<p><a href="https://duckduckgo.com/?q=how+to+organize+terraform+files">How to organize Terraform code</a> is a
question of constant debate, and the right answer is that there is no right answer. A lot of it
depends on how you organize your teams, so bear in mind that my cluster is maintained by a team of
one.</p>
<p>My module structure has one top-level module, with one module for each &ldquo;role&rdquo; that my nodes will
play:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>├── main.tf
</span></span><span style="display:flex;"><span>├── outputs.tf
</span></span><span style="display:flex;"><span>├── secrets.tfvars
</span></span><span style="display:flex;"><span>├── consul-server
</span></span><span style="display:flex;"><span>│   ├── main.tf
</span></span><span style="display:flex;"><span>│   ├── outputs.tf
</span></span><span style="display:flex;"><span>│   └── variables.tf
</span></span><span style="display:flex;"><span>├── nomad-client
</span></span><span style="display:flex;"><span>│   ├── main.tf
</span></span><span style="display:flex;"><span>│   ├── outputs.tf
</span></span><span style="display:flex;"><span>│   └── variables.tf
</span></span><span style="display:flex;"><span>├── nomad-server
</span></span><span style="display:flex;"><span>│   ├── main.tf
</span></span><span style="display:flex;"><span>│   ├── outputs.tf
</span></span><span style="display:flex;"><span>│   └── variables.tf
</span></span><span style="display:flex;"><span>└── vault-server
</span></span><span style="display:flex;"><span>    ├── main.tf
</span></span><span style="display:flex;"><span>    ├── outputs.tf
</span></span><span style="display:flex;"><span>    └── variables.tf</span></span></code></pre></div>
<p>Each of these modules has a number of variables in common, including how many instances to create,
which image to use when creating the node, and a couple of other values. Most of their inputs are
the same, but this provides a lot of flexibility, and common values are usually sourced from a block
of shared <code>locals</code>.</p>
<p>This setup has several advantages, primarily flexibility and a <a href="https://git.sr.ht/~damien/infrastructure/tree/master/terraform/main.tf">top-level
<code>main.tf</code></a> that is able to
describe the makeup of your cluster very cleanly, but the downside is that it is fairly verbose
within the module definitions. Terraform doesn&rsquo;t appear to provide any utilities for defining a
set of provisioners that can be shared across resources, which would help quite a bit.</p>
<h3 id="division-of-provision">Division of Provision</h3>
<p>The provisioning of a new node is split between a custom
<a href="https://git.sr.ht/~damien/infrastructure/tree/master/stackscripts/cluster-member.sh">stackscript</a>
and Terraform <a href="https://www.terraform.io/docs/provisioners/index.html">provisioners</a>. The stackscript
installs packages and does other configuration that is common across nodes, while the Terraform
provisioners are used to copy <a href="https://git.sr.ht/~damien/infrastructure/tree/master/config">configuration
files</a> up from the infra repo directly
and to write configuration files that are dependent on cluster knowledge, such as the addresses of
the Consul servers.</p>
<p>An alternative, and arguably better, setup would be to use <a href="https://www.packer.io/">Packer</a> to
define the node images, leaving nothing for Terraform to do except deploy instances and do the
little configuration that requires cluster knowledge. Unfortunately, this is an area where Linode
may not be a great choice; while Packer does support a Linode image builder, custom Linode images
don&rsquo;t appear to be compatible with their network helper tool, which causes basic networking to be
<a href="https://git.sr.ht/~damien/infrastructure/tree/c97e0e2b/packer/README.md">broken by default</a>.</p>
<h3 id="naming-things">Naming Things</h3>
<p>Initially, I took a very simple approach to naming nodes by their role, region, and an index, such
as <code>nomad-server-ca-central-1</code>. However, this approach lacks flexibility when it comes to upgrading
your cluster. If you want to replace a node, it is safest to create a new one and make sure it&rsquo;s up
and running before destroying the old one, but now your carefully numbered servers are no longer in
order.</p>
<p>Fortunately, Terraform provides a <a href="https://registry.terraform.io/providers/hashicorp/random/latest/docs">random
provider</a> that can be used to
name your nodes instead by generating random identifiers. I use something similar to this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-hcl" data-lang="hcl"><span style="display:flex;"><span><span style="color:#66d9ef">resource</span> <span style="color:#e6db74">&#34;random_id&#34; &#34;servers&#34;</span> {
</span></span><span style="display:flex;"><span>    count <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">servers</span>
</span></span><span style="display:flex;"><span>    keepers <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        datacenter     <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">datacenter</span>
</span></span><span style="display:flex;"><span>        image          <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">image</span>
</span></span><span style="display:flex;"><span>        instance_type  <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">instance_type</span>
</span></span><span style="display:flex;"><span>        consul_version <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">consul_version</span>
</span></span><span style="display:flex;"><span>        nomad_version  <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">nomad_version</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    byte_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">resource</span> <span style="color:#e6db74">&#34;linode_instance&#34; &#34;servers&#34;</span> {
</span></span><span style="display:flex;"><span>    count <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">servers</span>
</span></span><span style="display:flex;"><span>    label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;nomad-server-${var.datacenter}-${replace(random_id.servers[count.index].b64_url, &#34;-&#34;, &#34;_&#34;)}&#34;</span>
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>This gives each Nomad server a name like <code>nomad-server-ca-central-XXXXXX</code>, where <code>XXXXXX</code> is a
base64-encoded random string. The URL-safe base64-encoding is used, but Linode doesn&rsquo;t allow two
consecutive dashes in instance labels, so the <code>replace()</code> function is used to replace dashes with
underscores in order to prevent a provision failure caused by a dash as the first letter in a server
id. (It&rsquo;s happened to me once already, not a fun reason for the apply to fail)</p>
<h2 id="running-a-website">Running a Website</h2>
<p>At this point, we&rsquo;ve covered pretty much everything you need to be able to spin up a functional
cluster.  However, as I mentioned before, this blog is currently running on my own cluster, and
there are a number of extra steps that need to be taken in order to support running a website. In
this section, I will cover the points that are specific to running a website on this setup. While
web servers are similar to any other job type in many respects, there are a few additional concerns
that bear special mention.</p>
<h3 id="load-balancing">Load Balancing</h3>
<p>Running a website on Nomad makes it easy to scale up, but running more than one instance of a web
server requires some form of load balancer. The big name in load balancers is
<a href="http://www.haproxy.org/">HAProxy</a>, but a few newer ones can take advantage of Consul&rsquo;s
service-registration features in order to &ldquo;just work&rdquo; with no or minimal configuration. For this
website I chose <a href="https://fabiolb.net/">Fabio</a>, but <a href="https://docs.traefik.io/">Traefik</a> is another
good option.</p>
<p>Regardless of which you choose, you will then have to decide how to run it. Naturally, I decided to
run Fabio as a <a href="https://git.sr.ht/~damien/infrastructure/tree/master/jobs/fabio.nomad.erb">Nomad
job</a> too, but due to the
nature of load balancing, it has tighter restrictions for how it can run. Most jobs, including the
web server itself, don&rsquo;t actually care which nodes they run on, but load balancers need their host
to be registered with DNS. This means that we need the nodes themselves to know whether they are
intended to run a load balancer or not.</p>
<p>Nomad provides a number of filtering options for jobs including custom metadata, but I decided to go
with the <a href="https://www.nomadproject.io/docs/configuration/client#node_class"><code>node_class</code></a> attribute.
This is a custom value that you can asign each Nomad client explicitly for filtering purposes, and
has the added benefit over custom metadata of being included in node status output:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>damien@support:~&gt; nomad node status
</span></span><span style="display:flex;"><span>ID        DC          Name                            Class          Drain  Eligibility  Status
</span></span><span style="display:flex;"><span>e9d5cdfe  ca-central  nomad-client-ca-central-UgcT5Q  load-balancer  false  eligible     ready
</span></span><span style="display:flex;"><span>67d7b064  ca-central  nomad-client-ca-central-4XMmYQ  &lt;none&gt;         false  eligible     ready</span></span></code></pre></div>
<p>Fabio jobs can then be specified to run exclusively on <code>load-balancer</code> nodes with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-hcl" data-lang="hcl"><span style="display:flex;"><span><span style="color:#66d9ef">constraint</span> {
</span></span><span style="display:flex;"><span>	attribute <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;${node.class}&#34;</span>
</span></span><span style="display:flex;"><span>	value     <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;load-balancer&#34;</span>
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h3 id="dns-management">DNS Management</h3>
<p>Once the <code>load-balancer</code> node is up and running an instance of Fabio, everything should
<em>technically</em> be available on the internet, but it won&rsquo;t be very easy to reach without a domain
name. However, it would also be a pain to manually update a DNS management system with new records
every time your cluster changes.</p>
<p>Fortunately, DNS records can be considered just another part of your infrastructure, and can
therefore be provisioned with Terraform! This means that any time a new <code>load-balancer</code> node is
created or destroyed, a DNS record is created or destroyed along with it, automatically keeping your
domain name in sync with available load balancers.</p>
<p>To support this, I defined a Terraform module called
<a href="https://git.sr.ht/~damien/infrastructure/tree/master/terraform/domain-address"><code>domain-address</code></a>,
which takes as input the domain, a name for the record, and a list of Linode instances. The
<code>linode_domain_record</code> resource can then be used to define <code>A</code> and/or <code>AAAA</code> records pointing to the
IPv4 and/or IPv6 addresses respectively:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-hcl" data-lang="hcl"><span style="display:flex;"><span><span style="color:#66d9ef">data</span> <span style="color:#e6db74">&#34;linode_domain&#34; &#34;d&#34;</span> {
</span></span><span style="display:flex;"><span>  domain <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">domain</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">resource</span> <span style="color:#e6db74">&#34;linode_domain_record&#34; &#34;a&#34;</span> {
</span></span><span style="display:flex;"><span>  for_each    <span style="color:#f92672">=</span> toset(terraform.workspace <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;default&#34;</span> <span style="color:#960050;background-color:#1e0010">?</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">instances</span>[<span style="color:#960050;background-color:#1e0010">*</span>].<span style="color:#66d9ef">ip_address</span> <span style="color:#960050;background-color:#1e0010">:</span> [])
</span></span><span style="display:flex;"><span>  domain_id   <span style="color:#f92672">=</span> <span style="color:#66d9ef">data</span>.<span style="color:#66d9ef">linode_domain</span>.<span style="color:#66d9ef">d</span>.<span style="color:#66d9ef">id</span>
</span></span><span style="display:flex;"><span>  name        <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">name</span>
</span></span><span style="display:flex;"><span>  record_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;A&#34;</span>
</span></span><span style="display:flex;"><span>  target      <span style="color:#f92672">=</span> <span style="color:#66d9ef">each</span>.<span style="color:#66d9ef">value</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">resource</span> <span style="color:#e6db74">&#34;linode_domain_record&#34; &#34;aaaa&#34;</span> {
</span></span><span style="display:flex;"><span>  for_each    <span style="color:#f92672">=</span> toset(terraform.workspace <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;default&#34; ? [for ip in var.instances[*].ipv6 : split(&#34;/&#34;</span>, <span style="color:#66d9ef">ip</span>)[<span style="color:#ae81ff">0</span>]] <span style="color:#960050;background-color:#1e0010">:</span> [])
</span></span><span style="display:flex;"><span>  domain_id   <span style="color:#f92672">=</span> <span style="color:#66d9ef">data</span>.<span style="color:#66d9ef">linode_domain</span>.<span style="color:#66d9ef">d</span>.<span style="color:#66d9ef">id</span>
</span></span><span style="display:flex;"><span>  name        <span style="color:#f92672">=</span> <span style="color:#66d9ef">var</span>.<span style="color:#66d9ef">name</span>
</span></span><span style="display:flex;"><span>  record_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;AAAA&#34;</span>
</span></span><span style="display:flex;"><span>  target      <span style="color:#f92672">=</span> <span style="color:#66d9ef">each</span>.<span style="color:#66d9ef">value</span>
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>One thing to note here is the <code>terraform.workspace</code> check within the <code>for_each</code> line. This is to
support development flows that use <a href="https://www.terraform.io/docs/state/workspaces.html">Terraform
workspaces</a>, which can be useful for testing
cluster changes (such as OS upgrades) without affecting the existing deployment. DNS records are
global, so we use this check to ensure that they are only created within the default workspace and
aren&rsquo;t overwritten to point to a non-production cluster.</p>
<h3 id="cert-renewals">Cert Renewals</h3>
<p>The last step is to set up automatic SSL certificate renewal. If you don&rsquo;t need or want to serve
your website over HTTPS, then you can skip this step, but most websites should probably be served
securely and therefore will need SSL.</p>
<p>In addition to providing orchestration for always-on services, Nomad supports something akin to cron
jobs in the form of the
<a href="https://www.nomadproject.io/docs/job-specification/periodic.html"><code>periodic</code></a> stanza. With this, we
can write a Nomad job that executes our SSL renewal regularly so that its validity never lapses.</p>
<h4 id="getting-the-certificate">Getting the Certificate</h4>
<p>The first step is deciding which SSL renewal service and tool to go with. <a href="https://letsencrypt.org/">Let&rsquo;s
Encrypt</a> is the big name in this space because it&rsquo;s free and run by a
nonprofit, but that&rsquo;s not a hard requirement as long as whichever service you choose has APIs for
automatic renewal.</p>
<p>For tool, I decided to go with <a href="https://github.com/acmesh-official/acme.sh">acme.sh</a>, because it
provides a nice interface with minimal dependencies, though there are a number of <a href="https://letsencrypt.org/docs/client-options/">other
options</a> available for any ACME-compatible service.</p>
<h5 id="the-challenge">The Challenge</h5>
<p>The ACME protocol requires you to be able to prove that you own the domain being renewed through a
<a href="https://tools.ietf.org/html/rfc8555#section-8">challenge</a>, with the two main options being HTTP
and DNS. HTTP challenges work by giving you some data and verifying its existence under
<code>http://&lt;domain&gt;/.well-known/acme-challenge/</code>; DNS challenges work similarly, but the
challenge expects the data to be available as a TXT record on the domain.</p>
<p>Due to the distributed nature of jobs running on Nomad, the HTTP challenge is not really viable, so
I recommend using the DNS challenge along with your DNS provider&rsquo;s API, such as
<a href="https://github.com/acmesh-official/acme.sh/wiki/dnsapi#cloud-manager"><code>dns_linode_v4</code></a>.</p>
<h5 id="storing-certificates-in-vault">Storing Certificates in Vault</h5>
<p>Fabio supports loading SSL certificates from
<a href="https://github.com/fabiolb/fabio/wiki/Certificate-Stores#vault-kv">Vault</a>, so after the challenge
succeeds, that&rsquo;s where we will want to save the cert and key. However, this also becomes a little
tricky due to the distributed nature of this job, since Vault will be running on a different server.</p>
<p>Fortunately, Vault has an HTTP API, so as long as we have the address of at least one available
Vault server and a valid token, the job can send an HTTPS request with the new cert and key
contents. With <code>acme.sh</code>, this takes the form of specifying <code>--reloadcmd</code> and a script like
<a href="https://git.sr.ht/~damien/infrastructure/tree/master/artifacts/vault-write-certs.sh"><code>vault-write-certs.sh</code></a>,
which can be made available as a downloadable artifact that Nomad will make available to the renewal
job.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>The architecture described in this post was born out of a desire to better understand how cluster
networking works, a general interest in HashiCorp tools and philosophies, and a purely pragmatic
desire to be able to avoid cloud vendor lock-in when needed.</p>
<p>This post was written over the span of several months and ended up being pretty long, so it may not
be the easiest to read, and some of its links may also fall out of date as I continue making updates
to my architecture.</p>
<p>If you find anything that&rsquo;s broken, or you just have a question or comment, feel free to shoot me an
<a href="mailto:blog@damienradtke.com?subject=RE:%20Building%20a%20Cloud-Free%20Hashistack%20Cluster">✉️ email</a>.</p>
<!-- raw HTML omitted -->

  </div>

    </main>

    
  </body>
</html>
